
# FUNCTIONS FOR DENSITY/DISTRIBUTION --------------------------------------

#' Calculate and Combine Density Estimates for Observed and Predicted Data
#'
#' Internal helper functions that return a `data.frame` summarizing density
#' values for observed and predicted response times.
#'
#' @details
#' `calc_dens_obs()` computes empirical histograms and kernel density
#' estimates for a single condition based on observed RTs.
#'
#' `calc_dens()` serves as a general interface that combines observed and
#' predicted data into a single `data.frame`. Observed data (`rts_u` and
#' `rts_l`) is passed to `calc_dens_obs()`. Predicted data (`pdf_u` and
#' `pdf_l`) is wrapped into a `data.frame` that matches the structure returned
#' by `calc_dens_obs()`. If both are provided, observed and predicted data
#' are row-bound into a single `data.frame`.
#'
#' These functions are used internally to support `type = "density"` in
#' [dRiftDM::calc_stats()], providing a full distributional overview.
#'
#' @inheritParams calc_quantiles
#' @param discr a single numeric value defining the bin width for histogram and
#'   KDE estimation. Defaults to 0.015 (seconds).
#' @param t_max a single numeric value specifying the maximum RT to consider.
#'   Defaults to the smallest multiple of `discr` above the maximum RT. If
#'   `t_vec` is provided, `t_max` defaults to the maximum value of `t_vec`.
#' @param scaling_factor a single numeric value, multiplied with the PDFs.
#'   It is used to scale the corresponding probability mass
#'   proportional to the number of trials per condition. Defaults to `1.0`.
#'
#' @returns
#' A `stats_dm` object (via [dRiftDM::new_stats_dm()]) containing a
#' `data.frame` with columns:
#' - `Source`: indicates whether the row is from observed (`"obs"`) or
#'   predicted (`"pred"`) data.
#' - `Cond`: the condition label.
#' - `Time`: the time point corresponding to the density value.
#' - `Stat`: type of density summaryâ€”`"hist"` or `"kde"` (for observed data),
#'   or `"pdf"` (for predicted data).
#' - `Dens_<U>`: density value for the upper response.
#' - `Dens_<L>`: density value for the lower response.
#'
#' The `<U>` and `<L>` placeholders are determined by the `b_coding` argument.
#'
#' @keywords internal
#' @name calc_dens_functions
calc_dens_obs = function(rts_u, rts_l, one_cond, t_max = NULL, discr = NULL,
                         scaling_factor = 1.0) {

  # ensure that t_max and discr arguments are reasonable
  max_rt = max(rts_u, rts_l)
  if (is.null(discr)) {
    discr = 0.015
  }
  if (is.null(t_max)) {
    t_max = max_rt
  }
  stopifnot(discr < t_max)

  # Align t_max with discr steps
  t_max = ceiling(t_max / discr) * discr
  stopifnot(max_rt < t_max)

  # define the breaks (taking into account the maximum RT value)
  n = round(t_max / discr + 1)
  breaks = seq(from = 0, to = t_max, length.out = n)
  mids <- breaks[-length(breaks)] + diff(breaks) / 2

  # call hist() twice and extract the values
  dens_hist_u = hist(rts_u, breaks = breaks, plot = FALSE)$density
  dens_hist_l = hist(rts_l, breaks = breaks, plot = FALSE)$density

  # call density() twice and extract the values
  # the wrapper ensures that the function doesn't crash if one of the
  # rt vectors has less than two values
  kde_wrapper <- function(vals) {
    if (length(vals) <= 1) return(rep(NaN, length(mids)))
    d <- density(vals, from = 0, to = t_max)
    d <- approx(d$x, d$y, xout = mids, rule = 2)$y
    d <- d / sum(d * discr) # ensure it sums to 1
    return(d)
  }
  dens_kde_u = kde_wrapper(rts_u)
  dens_kde_l = kde_wrapper(rts_l)


  # calculate the relative contribution
  ratio = length(rts_u) / (length(rts_u) + length(rts_l))

  # pack up as a data.frame, scale and return
  dens_stats <- data.frame(
    Cond = one_cond,
    Stat = rep(c("hist", "kde"), each = length(mids)),
    Time = rep(mids, 2),
    Dens_U = c(dens_hist_u, dens_kde_u) * ratio * scaling_factor,
    Dens_L = c(dens_hist_l, dens_kde_l) * (1 - ratio) * scaling_factor
  )

  return(dens_stats)
}


#' @rdname calc_dens_functions
calc_dens <- function(pdf_u = NULL, pdf_l = NULL, t_vec = NULL, t_max = NULL,
                      discr = NULL, rts_u = NULL, rts_l = NULL, one_cond,
                      b_coding, scaling_factor = 1.0) {

  # parameter extraction
  u_name <- names(b_coding$u_name_value)
  l_name <- names(b_coding$l_name_value)

  # check pdf_l and pdf_u
  if (xor(is.null(pdf_l), is.null(pdf_u))) {
    stop("pdf_l and pdf_u either have to be both NULL or not")
  }

  # check rts_u and rts_l
  if (xor(is.null(rts_u), is.null(rts_l))) {
    stop("rts_u and rts_l either have to be both NULL or not")
  }

  # wrap up the pdfs when they are supplied ...
  result_pred <- NULL
  if (!is.null(pdf_u)) {

    stopifnot(length(pdf_u) == length(pdf_l))
    stopifnot(length(pdf_u) == length(t_vec))

    result_pred <- data.frame(
      Source = "pred",
      Cond = one_cond,
      Stat = "pdf",
      Time = t_vec,
      Dens_U = pdf_u * scaling_factor,
      Dens_L = pdf_l * scaling_factor
    )
  }


  # if rts are supplied, calculate the density statistics
  result_obs <- NULL
  if (!is.null(rts_u)) {

    # if there is no specified t_max value, check for temp options
    if (is.null(t_max)) {
      t_max = stats.options("t_max") # returns NULL or some value
    }

    # if there is a time vector, extract t_max and use it (overrides any input
    # or options)
    if (!is.null(t_vec)) {
      t_max = max(t_vec)
    }

    result_obs <- calc_dens_obs(
      rts_u = rts_u, rts_l = rts_l, one_cond = one_cond, t_max = t_max,
      discr = discr, scaling_factor = scaling_factor
    )
    result_obs <- cbind(Source = "obs", result_obs)
  }

  # maybe combine
  result <- rbind(result_obs, result_pred)


  # rename
  if (!is.null(result)) {
    colnames(result)[which(names(result) == "Dens_U")] <-
      paste("Dens", u_name, sep = "_")
    colnames(result)[which(names(result) == "Dens_L")] <-
      paste("Dens", l_name, sep = "_")
  }


  # make it dm_* class and pass back
  stats_dm_obj <- new_stats_dm(
    stat_df = result, type = "densities",
    b_coding = b_coding
  )
  return(stats_dm_obj)
}


# FUNCTIONS FOR BASIC SUMMARY STATISTICS ------------------------------------

#' Calculate Basic Mean, Standard Deviations, and Percentages for Observed and
#' Predicted Data
#'
#' Backend functions to compute the mean response times, standard deviations of
#' response times, and response proportions; for both observed RTs or the
#' predicted probability density functions.
#'
#' @param rts_u,rts_l numeric, vectors of response times for the upper and lower
#'  boundaries.
#' @param pdf_u,pdf_l numeric, vectors of probability density values for the
#'  upper and lower boundaries.
#' @param one_cond character, a label for the condition.
#' @param t_vec numeric vector, containing the time points corresponding to the
#'  probability density values.
#' @param dt a single numeric, providing the step size in `t_vec`.
#' @param skip_if_contr_low a single numeric, threshold below which probability
#' densities are ignored (default is obtained from
#' [dRiftDM::drift_dm_skip_if_contr_low()]).
#'
#' @returns A [data.frame] with columns:
#'   - `Cond`: Condition label.
#'   - `Mean_U`: Mean response time for the upper boundary
#'   - `Mean_L`: Mean response time for the lower boundary
#'   - `SD_U`: Mean response time for the upper boundary
#'   - `SD_L`: Mean response time for the lower boundary
#'   - `P_U`: Proportion of upper-boundary responses.
#'
#' @details
#' - For observed data, calculates mean RTs, standard deviations of RTs, and
#'   the proportion of upper responses.
#' - The same statistics are calculated for the probability density values (via
#'   simple numerical integration)
#'
#' @keywords internal
calc_basic_stats_obs <- function(rts_u, rts_l, one_cond) {

  # calculate the means
  mean_u = mean(rts_u)
  mean_l = mean(rts_l)

  # calculate the sds
  sd_u = stats::sd(rts_u)
  sd_l = stats::sd(rts_l)

  # calculate the percentages (e.g., error rates)
  ratio = length(rts_u) / (length(rts_u) + length(rts_l))

  # pack up as a data.frame and return
  basic_stats <- data.frame(
    Cond = one_cond,
    Mean_U = mean_u,
    Mean_L = mean_l,
    SD_U = sd_u,
    SD_L = sd_l,
    P_U = ratio
  )
  return(basic_stats)
}


#' @rdname calc_basic_stats_obs
calc_basic_stats_pred <- function(pdf_u, pdf_l, one_cond, t_vec, dt,
                                  skip_if_contr_low = NULL) {

  stopifnot(length(pdf_u) == length(pdf_l))
  stopifnot(length(pdf_u) == length(t_vec))
  stopifnot(!is.null(dt))

  if (is.null(skip_if_contr_low))
    skip_if_contr_low <- drift_dm_skip_if_contr_low()

  n_steps <- length(pdf_u)

  # first, integrate to cdfs
  cdf_u <- cumtrapz(x = t_vec, y = pdf_u)
  cdf_l <- cumtrapz(x = t_vec, y = pdf_l)

  # to calculate the ratio of the probability mass
  sum_pdf_u <- cdf_u[n_steps]
  sum_pdf_l <- cdf_l[n_steps]
  ratio = sum_pdf_u / (sum_pdf_l + sum_pdf_u)

  # determine if the contribution is relevant
  if (sum_pdf_u < skip_if_contr_low) pdf_u <- rep(NA, length(pdf_u))
  if (sum_pdf_l < skip_if_contr_low) pdf_l <- rep(NA, length(pdf_l))

  # then scale each pdf
  pdf_u = pdf_u / sum_pdf_u
  pdf_l = pdf_l / sum_pdf_l

  # then calculate the mean
  tmp_u = cumtrapz(x = t_vec, y = t_vec*pdf_u)
  mean_u = tmp_u[n_steps]
  tmp_l = cumtrapz(x = t_vec, y = t_vec*pdf_l)
  mean_l = tmp_l[n_steps]

  # and the standard deviation (V(X) = E(X^2) - E(X)^2)
  tmp_u = cumtrapz(x = t_vec, y = t_vec^2*pdf_u)
  sd_u = sqrt(tmp_u[n_steps] - mean_u^2)
  tmp_l = cumtrapz(x = t_vec, y = t_vec^2*pdf_l)
  sd_l = sqrt(tmp_l[n_steps] - mean_l^2)

  # pass back
  basic_stats <- data.frame(
    Cond = one_cond,
    Mean_U = mean_u,
    Mean_L = mean_l,
    SD_U = sd_u,
    SD_L = sd_l,
    P_U = ratio
  )
  return(basic_stats)
}




#' Calculate Basic Statistics for Response Times or Probability Densities
#'
#' Function that calls the underlying functions
#' [dRiftDM::calc_basic_stats_obs] and [dRiftDM::calc_basic_stats_pred].
#' Handles input checks and data wrangling.
#'
#' @inheritParams calc_basic_stats_obs
#' @param b_coding list, used for accessing the upper boundary label, determines
#' the corresponding column of the returned data.frame (e.g., P_`corr`).
#'
#' @return A [data.frame] with columns:
#'   - `Source`: Indicates whether the statistics refer to observed (`obs`) or
#'    predicted (`pred`) data.
#'   - `Cond`: A condition label.
#'   - `Mean_<u_label>`: Mean response time for the upper boundary.
#'   - `Mean_<l_label>`: Mean response time for the lower boundary.
#'   - `P_<u_label>`: Proportion of upper-boundary responses.
#'
#'
#' @details
#' - If `pdf_u` and `pdf_l` are provided, the function computes statistics for
#'   the probability densities.
#' - If `rts_u` and `rts_l` are provided, the function computes statistics for
#'   the observed RTs.
#' - If both sets of inputs are provided, both types of statistics are computed
#'   and returned.
#'
#' @seealso [dRiftDM::calc_basic_stats_obs], [dRiftDM::calc_basic_stats_pred],
#' [dRiftDM::new_stats_dm]
#'
#' @keywords internal
calc_basic_stats <- function(pdf_u = NULL, pdf_l = NULL, rts_u = NULL,
                             rts_l = NULL, one_cond, b_coding, t_vec = NULL,
                             dt = NULL, skip_if_contr_low = NULL) {


  # boundary settings and checks
  u_name <- names(b_coding$u_name_value)
  l_name <- names(b_coding$l_name_value)


  # check pdf_l and pdf_u
  if (xor(is.null(pdf_l), is.null(pdf_u))) {
    stop("pdf_l and pdf_u either have to be both NULL or not")
  }

  # check rts_u and rts_l
  if (xor(is.null(rts_u), is.null(rts_l))) {
    stop("rts_u and rts_l either have to be both NULL or not")
  }

  # calculate the respective statistics
  # if pdfs are supplied ...
  result_pred <- NULL
  if (!is.null(pdf_u)) {
    result_pred <- calc_basic_stats_pred(
      pdf_u = pdf_u, pdf_l = pdf_l, one_cond = one_cond,
      t_vec = t_vec, dt = dt, skip_if_contr_low = skip_if_contr_low
    )
    result_pred <- cbind(Source = "pred", result_pred)

  }

  result_obs <- NULL
  if (!is.null(rts_u)) {
    result_obs <- calc_basic_stats_obs(
      rts_u = rts_u, rts_l = rts_l, one_cond = one_cond
    )
    result_obs <- cbind(Source = "obs", result_obs)
  }



  # combine and format names
  result <- rbind(result_obs, result_pred)

  if (!is.null(result)) {
    new_names_u = gsub(pattern = "_U$", replacement = paste0("_", u_name),
                       x = grep("_U$", names(result), value = T))
    colnames(result)[grepl(pattern = "_U$", x = names(result))] <-
      new_names_u

    new_names_l = gsub(pattern = "_L$", replacement = paste0("_", l_name),
                       x = grep("_L$", names(result), value = T))
    colnames(result)[grepl(pattern = "_L$", x = names(result))] <-
      new_names_l
  }

  # make it a stats_dm class and pass back
  stats_dm_obj <- new_stats_dm(
    stat_df = result, type = "basic_stats",
    b_coding = b_coding
  )
  return(stats_dm_obj)
}


# FUNCTIONS FOR CALCULATING THE CAF ---------------------------------------

#' Calculate CAFs
#'
#' Backend functions to calculate conditional accuracy functions for RT
#' vectors or pdfs
#'
#' @param rts_u,rts_l vectors of RTs for the upper and lower boundary
#' @param pdf_u,pdf_l density values for the upper and lower boundary
#' @param t_vec the time space
#' @param one_cond character label
#' @param n_bins number of bins to use for the CAFs
#'
#' @returns a data.frame with the "Cond" label, the "Bin"s and "P_U" for
#' the CAFs
#'
#' @details
#' for RTs:
#' first elements are attributed to a bin (with bins calculated across all RTs
#' using equally spaced quantiles),
#' then accuracy per bin is calculated.
#'
#' for Densities: Add density values, calculate a CDF and force it between 0 and
#' 1. Then determine the indices that cut the CDF into bins by considering
#' equally spaced quantiles. Then calculate the ratio of probability mass per
#' bin.
#'
#' @keywords internal
calc_cafs_obs <- function(rts_u, rts_l, one_cond, n_bins) {
  # cut (all) RTs into n bins
  rts <- c(rts_u, rts_l)
  probs <- seq(0, 1, length.out = n_bins + 1)
  borders <- stats::quantile(rts, probs = probs)
  bins <- cut(rts, breaks = borders, labels = FALSE, include.lowest = TRUE)
  stopifnot(identical(sort(unique(bins)), 1:n_bins))

  # create a vector to code accuracy
  corr <- rep(c(1, 0), times = c(length(rts_u), length(rts_l)))

  # separate accuracy vector by the calcualted bins, and get mean accuracy per
  # bin
  caf <- tapply(corr, bins, mean)
  caf <- data.frame(
    Cond = one_cond,
    Bin = as.numeric(names(caf)),
    P_U = as.numeric(caf)
  )
  return(caf)
}

#' @rdname calc_cafs_obs
calc_cafs_pred <- function(pdf_u, pdf_l, t_vec, one_cond, n_bins) {
  stopifnot(length(pdf_u) == length(pdf_l))
  stopifnot(length(pdf_u) == length(t_vec))

  # make a cdf
  cdf <- cumtrapz(x = t_vec, y = pdf_u + pdf_l)
  cdf <- cdf / max(cdf)

  # get the quantiles that determine the bins
  probs <- seq(0, 1, length.out = n_bins + 1)
  probs <- probs[2:(length(probs) - 1)]

  # get the indices that determine the "cuts"
  x <- 1:length(cdf)
  x_borders <- stats::approx(x = cdf, y = x, xout = probs, ties = "mean")$y
  x_borders <- append(x_borders, min(x), after = 0) # ensure that x_borders
  x_borders <- append(x_borders, max(x)) # contains the lower and upper part

  # "label" the indices by the bins
  bins <- cut(x, breaks = x_borders, labels = FALSE, include.lowest = TRUE)
  stopifnot(identical(unique(bins), 1:n_bins))

  # determine the probability mass per bin and pdf
  sum_u <- tapply(pdf_u, bins, sum)
  sum_l <- tapply(pdf_l, bins, sum)

  # calculate the ratio and pass back
  caf <- sum_u / (sum_u + sum_l)
  caf <- data.frame(
    Cond = one_cond,
    Bin = as.numeric(names(caf)),
    P_U = as.numeric(caf)
  )
  return(caf)
}

# internal function for input checking, data wrangling, and default values
#' Calculate CAFs
#'
#' Function that calls the underlying CAF calculation functions
#' [dRiftDM::calc_cafs_obs] and [dRiftDM::calc_cafs_pred]. Does input checks
#' and the data wrangling
#'
#' @inheritParams calc_cafs_obs
#' @param b_coding used for accessing the upper boundary label, determines
#' the corresponding column of the returned data.frame (e.g., P_`corr`).
#'
#' @returns a data.frame with "Source", "Cond", "Bin"s, "P_<u_label>" for
#' the CAFs of type c("cafs", "sum_dist", "stats_dm", "data.frame")
#'
#' @details
#' if pdf_u and pdf_l are not NULL, returns CAFs of the densities
#'
#' if rts_u and rts_l are not NULL, returns CAFs of the response times
#'
#' if all are not NULL, returns both.
#'
#' @seealso [dRiftDM::new_stats_dm()]
#'
#' @keywords internal
calc_cafs <- function(pdf_u = NULL, pdf_l = NULL, t_vec = NULL,
                      rts_u = NULL, rts_l = NULL, one_cond, n_bins = NULL,
                      b_coding) {
  # default settings and parameter extraction
  if (is.null(n_bins)) {
    n_bins <- drift_dm_default_n_bins()
  }

  if (!is_numeric(n_bins) | length(n_bins) != 1) {
    stop("n_bins must a valid numeric")
  }
  if (n_bins <= 1) {
    stop("argument n_bins nust be larger than 1")
  }

  u_name <- names(b_coding$u_name_value)


  # check pdf_l and pdf_u
  if (xor(is.null(pdf_l), is.null(pdf_u))) {
    stop("pdf_l and pdf_u either have to be both NULL or not")
  }

  # check rts_u and rts_l
  if (xor(is.null(rts_u), is.null(rts_l))) {
    stop("rts_u and rts_l either have to be both NULL or not")
  }

  # calculate the respective statistics
  # if pdfs are supplied ...
  result_pred <- NULL
  if (!is.null(pdf_u)) {
    result_pred <- calc_cafs_pred(
      pdf_u = pdf_u, pdf_l = pdf_l, t_vec = t_vec,
      one_cond = one_cond, n_bins = n_bins
    )
    result_pred <- cbind(Source = "pred", result_pred)
    colnames(result_pred)[which(names(result_pred) == "P_U")] <-
      paste("P", u_name, sep = "_")
  }

  result_obs <- NULL
  if (!is.null(rts_u)) {
    result_obs <- calc_cafs_obs(
      rts_u = rts_u, rts_l = rts_l,
      one_cond = one_cond, n_bins = n_bins
    )
    result_obs <- cbind(Source = "obs", result_obs)
    colnames(result_obs)[which(names(result_obs) == "P_U")] <-
      paste("P", u_name, sep = "_")
  }

  # combine
  result <- rbind(result_obs, result_pred)

  # make it stats_dm class and pass back
  stats_dm_obj <- new_stats_dm(
    stat_df = result, type = "cafs",
    b_coding = b_coding
  )
  return(stats_dm_obj)
}






# FUNCTIONS FOR CALCULATING QUANTILES -------------------------------------

#' Calculate Quantiles
#'
#' Backend functions to calculate quantiles for RT vectors or pdfs
#'
#' @param rts_u,rts_l vectors of RTs for the upper and lower boundary
#' @param pdf_u,pdf_l density values for the upper and lower boundary
#' @param one_cond character label
#' @param probs numeric vector with values between 0 and 1 for the probability
#' levels
#' @param t_vec the time space (required for the pdfs)
#' @param dt the step size corresponding to the time space
#'
#' @param skip_if_contr_low numeric. If the contribution of the upper
#' or lower PDF to the overall PDF is too low, return NAs for
#' this PDF (see also [dRiftDM::drift_dm_skip_if_contr_low()]).
#'
#' @returns a data.frame with the "Cond" label, the "Prob"s and "Quant_U" and
#' "Quant_L" for the quantiles
#'
#' @details
#' for RTs:
#' straightforward via [stats::quantile].
#'
#' for Densities: Calculate CDF (for each pdf separately here), and then map
#' the desired probability level via the CDF (y-axis) to the time space (x-axis)
#'
#' @keywords internal
calc_quantiles_obs <- function(rts_u, rts_l, one_cond, probs) {
  quants_rts_u <- stats::quantile(rts_u, probs = probs)
  quants_rts_l <- stats::quantile(rts_l, probs = probs)
  quants <- data.frame(
    Cond = one_cond,
    Prob = probs,
    Quant_U = unname(quants_rts_u),
    Quant_L = unname(quants_rts_l)
  )
  return(quants)
}

#' @rdname calc_quantiles_obs
calc_quantiles_pred <- function(pdf_u, pdf_l, t_vec, one_cond, probs, dt,
                                skip_if_contr_low = NULL) {
  stopifnot(length(pdf_u) == length(pdf_l))
  stopifnot(length(pdf_u) == length(t_vec))
  stopifnot(!is.null(dt))

  if (is.null(skip_if_contr_low))
    skip_if_contr_low <- drift_dm_skip_if_contr_low()

  quants <-
    apply(cbind(pdf_u, pdf_l), MARGIN = 2, function(a_pdf, t_vec, probs) {
      cdf <- cumtrapz(x = t_vec, y = a_pdf)
      cdf <- cdf / max(cdf)
      return(stats::approx(x = cdf, y = t_vec, xout = probs, ties = "mean")$y)
    }, t_vec = t_vec, probs = probs)

  colnames(quants) <- c("Quant_U", "Quant_L")

  quants <- as.data.frame(quants)
  quants <- cbind(Cond = one_cond, Prob = probs, quants)

  sum_pdf_l <- sum(pdf_l)
  sum_pdf_u <- sum(pdf_u)

  if (sum_pdf_u * dt < skip_if_contr_low) {
    quants["Quant_U"] <- NA
  }
  if (sum_pdf_l * dt < skip_if_contr_low) {
    quants["Quant_L"] <- NA
  }

  return(quants)
}


#' Calculate Quantiles
#'
#' Function that calls the underlying quantile calculation functions
#' [dRiftDM::calc_quantiles_obs()] and [dRiftDM::calc_quantiles_pred()]. Does
#' input checks and the data wrangling
#'
#' @inheritParams calc_quantiles_obs
#' @param b_coding used for accessing the upper/lower boundary labels,
#' determines the corresponding columns of the returned data.frame
#' (e.g., Quant_`corr`).

#'
#' @returns a data.frame with "Source", "Cond", "Prob"s, "Quant_<u_label>",
#' "Quant_<l_label>" of type
#' c("quantiles", "sum_dist", "stats_dm", "data.frame")
#'
#' @details
#' if pdf_u and pdf_l are not NULL, returns quantiles for the densities
#'
#' if rts_u and rts_l are not NULL, returns quantiles for the response times
#'
#' if all are not NULL, returns both.
#'
#' @seealso [dRiftDM::new_stats_dm()]
#'
#' @keywords internal
calc_quantiles <- function(pdf_u = NULL, pdf_l = NULL, t_vec = NULL, dt = NULL,
                           rts_u = NULL, rts_l = NULL, one_cond,
                           probs = NULL, b_coding, skip_if_contr_low = NULL) {
  # default settings and parameter extraction
  if (is.null(probs)) {
    probs <- drift_dm_default_probs()
  }

  if (!is.numeric(probs) | length(probs) < 2) {
    stop("probs must a valid numeric vector of length > 1")
  }

  if (min(probs) <= 0 | max(probs) >= 1) {
    stop("argument probs must be in the range ]0, 1[")
  }

  u_name <- names(b_coding$u_name_value)
  l_name <- names(b_coding$l_name_value)

  # check pdf_l and pdf_u
  if (xor(is.null(pdf_l), is.null(pdf_u))) {
    stop("pdf_l and pdf_u either have to be both NULL or not")
  }

  # check rts_u and rts_l
  if (xor(is.null(rts_u), is.null(rts_l))) {
    stop("rts_u and rts_l either have to be both NULL or not")
  }

  # calculate the respective statistics
  # if pdfs are supplied ...
  result_pred <- NULL
  if (!is.null(pdf_u)) {
    result_pred <- calc_quantiles_pred(
      pdf_u = pdf_u, pdf_l = pdf_l,
      t_vec = t_vec, dt = dt, one_cond = one_cond,
      probs = probs, skip_if_contr_low = skip_if_contr_low
    )
    result_pred <- cbind(Source = "pred", result_pred)

    colnames(result_pred)[which(names(result_pred) == "Quant_U")] <-
      paste("Quant", u_name, sep = "_")
    colnames(result_pred)[which(names(result_pred) == "Quant_L")] <-
      paste("Quant", l_name, sep = "_")
  }


  # if rts are supplied ...
  result_obs <- NULL
  if (!is.null(rts_u)) {
    result_obs <- calc_quantiles_obs(
      rts_u = rts_u, rts_l = rts_l,
      one_cond = one_cond, probs = probs
    )
    result_obs <- cbind(Source = "obs", result_obs)

    colnames(result_obs)[which(names(result_obs) == "Quant_U")] <-
      paste("Quant", u_name, sep = "_")
    colnames(result_obs)[which(names(result_obs) == "Quant_L")] <-
      paste("Quant", l_name, sep = "_")
  }


  # maybe combine
  result <- rbind(result_obs, result_pred)

  #make it dm_* class and pass back
  stats_dm_obj <- new_stats_dm(
    stat_df = result, type = "quantiles",
    b_coding = b_coding
  )
  return(stats_dm_obj)
}




#' Calculate delta function(s)
#'
#' Given a dataset providing the quantiles ([dRiftDM::calc_quantiles]),
#' calculates delta function(s) for the character vectors minuends and
#' subtrahends
#'
#' @param quantiles_dat a data.frame of quantiles ([dRiftDM::calc_quantiles])
#' @param minuends,subtrahends character vectors (with equal length), specifying
#' the conditions to use for the delta function: minuend - subtrahend
#' @param dvs character, indicating which quantile columns to use. Default
#' is "Quant_<u_label>". If multiple dvs are provided, then minuends and
#' subtrahends must have the same length, and matching occurs pairwise. In this
#' case, if only one minuend/subtrahend is specified, minuend and subtrahend are
#' recycled to the necessary length.
#' @param b_coding a [dRiftDM::b_coding] object, necessary to build default
#' dvs
#'
#' @details
#' Takes the quantile data_frame, [stats::reshape] it to wide, and then
#' access the relevant `dv` columns, together with minuends and subtrahends
#' to calculate the delta functions.
#'
#' @returns a data.frame with columns "Source", "Prob", the "Quant_<u_label>",
#' "Quant_<l_label". May have the following additional columns:
#'
#'  * if only one dv: as many Delta_<minuend_subtrahend> and
#'  Avg_<minuends_subtrahends> as minuends and subtrahends.
#'
#'  * if more than one dv: as many Delta_<u/l-label>_<minuend_subtrahend> and
#'  Avg_<u/l-label>_<minuends_subtrahends> as minuends and subtrahends.
#'
#'  The data.frame is of type
#'  c("delta_funs", "sum_dist", "stats_dm", "data.frame")
#'
#'
#' @seealso [dRiftDM::new_stats_dm()]
#'
#' @keywords internal
calc_delta_funs <- function(quantiles_dat, minuends = NULL, subtrahends = NULL,
                            dvs = NULL, b_coding) {
  # input checks on data frame
  if (!is.data.frame(quantiles_dat)) {
    stop("the provided quantiles_dat is not a data.frame")
  }

  u_name <- names(b_coding$u_name_value)
  l_name <- names(b_coding$l_name_value)
  quant_name_u <- paste("Quant", u_name, sep = "_")
  quant_name_l <- paste("Quant", l_name, sep = "_")

  nec_columns <- c("Source", "Cond", "Prob", quant_name_u, quant_name_l)
  if (any(colnames(quantiles_dat) != nec_columns)) {
    stop(
      "the provided quantiles_dat provides unexpected column names",
      "\n\tprovided: ", paste(colnames(quantiles_dat), collapse = " "),
      "\n\tnecessary: ", paste(nec_columns, collapse = " ")
    )
  }

  # input checks on minuends/subtrahends
  if (is.null(minuends)) {
    stop("calc_delta_funs was called but the argument minuends not provided")
  }
  if (is.null(subtrahends)) {
    stop("calc_delta_funs was called but the argument subtrahends not provided")
  }
  if (!is.character(minuends) | length(minuends) < 1) {
    stop("minuends must be a character vector of length >= 1")
  }
  if (!is.character(subtrahends) | length(subtrahends) < 1) {
    stop("subtrahends must be a character vector of length >= 1")
  }
  if (length(subtrahends) != length(minuends)) {
    stop("different length of minuends and subtrahends")
  }
  if (!all(minuends %in% unique(quantiles_dat$Cond))) {
    stop("Conds specified in minuends are not provided within quantiles_dat")
  }
  if (!all(subtrahends %in% unique(quantiles_dat$Cond))) {
    stop("Conds specified in subtrahends are not provided within quantiles_dat")
  }

  # input checks on dvs
  if (is.null(dvs)) {
    dvs <- quant_name_u
  }
  dvs <- sapply(dvs, function(x) {
    match.arg(x, c(quant_name_u, quant_name_l))
  })
  dvs <- unname(dvs)

  if (length(dvs) > 1 & length(dvs) != length(minuends)) {
    if (length(minuends) == 1) {
      minuends <- rep(minuends, length(dvs))
      subtrahends <- rep(subtrahends, length(dvs))
    } else {
      stop(
        "if several dvs are provided, the length must match",
        " minuends/subtrahends"
      )
    }
  }

  # reduce and make wide format
  quantiles_dat <- quantiles_dat[c("Source", "Cond", "Prob", dvs)]

  n_probs <- length(unique(quantiles_dat$Prob))
  n_source <- length(unique(quantiles_dat$Source))
  n_cond <- length(unique(quantiles_dat$Cond))
  if (nrow(quantiles_dat) != n_probs * n_source * n_cond) {
    stop(
      "quantiles_dat doesn't uniquely code rows solely by Probs, Source, ",
      "and Cond"
    )
  }
  quantiles_dat <- stats::reshape(quantiles_dat,
    idvar = c("Source", "Prob"),
    timevar = "Cond", direction = "wide", sep = "_"
  )

  # calculate delta functions
  if (length(dvs) == 1) {
    delta_names <- paste("Delta",
      paste(minuends, subtrahends, sep = "_"),
      sep = "_"
    )
    avg_names <- paste("Avg",
      paste(minuends, subtrahends, sep = "_"),
      sep = "_"
    )
  } else {
    delta_names <- paste("Delta", gsub("^Quant_", "", dvs), sep = "_")
    avg_names <- paste("Avg", gsub("^Quant_", "", dvs), sep = "_")
    delta_names <- paste(delta_names,
      paste(minuends, subtrahends, sep = "_"),
      sep = "_"
    )
    avg_names <- paste(avg_names,
      paste(minuends, subtrahends, sep = "_"),
      sep = "_"
    )
  }
  minuends_wide <- paste(dvs, minuends, sep = "_")
  subtrahends_wide <- paste(dvs, subtrahends, sep = "_")


  for (i in seq_along(minuends)) {
    vals_minuends <- quantiles_dat[[minuends_wide[i]]]
    vals_subtrahends <- quantiles_dat[[subtrahends_wide[i]]]

    quantiles_dat[[delta_names[i]]] <- vals_minuends - vals_subtrahends
    quantiles_dat[[avg_names[i]]] <- 0.5 * vals_minuends + 0.5 * vals_subtrahends
  }

  # make it stats_dm class and pass back
  # since delta_funs depends on quantiles, this will remove the quantiles class
  stats_dm_obj <- new_stats_dm(
    stat_df = quantiles_dat, type = "delta_funs",
    b_coding = b_coding
  )
  return(stats_dm_obj)
}



# FUNCTIONS FOR FIT STATISTICS --------------------------------------------



#' Calculate Fit Statistics
#'
#' Computes/Summarizes the current value of the cost function. In case the
#' cost function is a log-likelihood, then the Log-Likelihood, the Negative
#' Log-Likelihood, the Akaike Information Criterion (AIC), and the Bayesian
#' Information Criterion (BIC) are returned. In case the cost function is
#' the Root-Mean-Squared Error (RMSE) statistic, the respective statistic
#' in the unit of milliseconds and seconds is returned.
#'
#' @param drift_dm_obj an object of type [dRiftDM::drift_dm]
#' @param k a single numeric, scaling the penality of [stats::AIC])
#'
#' @return A custom object of class `stats_dm`
#' (c("fit_stats", "stats_dm", "data.frame")). The columns are either:
#' * `Log_Like`: the log-likelihood value
#' * `Neg_Log_Like`: the negative log-likelihood value
#' * `AIC`: the calculated AIC value
#' * `BIC`: the calculated BIC value
#' or
#' * `RMSE_s`: the root-mean-squared error (for RTs in seconds)
#' * `RMSE_ms`: the root-mean-squared error (for RTs in milliseconds)
#'
#' @seealso [dRiftDM::new_stats_dm()], [dRiftDM::logLik.drift_dm]
#'
#' @keywords internal
calc_fit_stats <- function(drift_dm_obj, k = 2) {

  cost_function <- cost_function(drift_dm_obj)
  stopifnot(cost_function %in% drift_dm_cost_functions())

  if (cost_function == "neg_log_like") {
    ll <- logLik(drift_dm_obj) # returns log-like (not the negative log-like)
    if (is.null(ll)) return (NULL)
    neg_ll <- ll * -1.0
    aic <- stats::AIC(ll, k = k)
    bic <- stats::BIC(ll)
    result <- data.frame(
      Log_Like = ll,
      Neg_Log_Like = neg_ll,
      AIC = aic,
      BIC = bic
    )
  }

  if (cost_function == "rmse") {
    rmse <- cost_value(drift_dm_obj)
    if (is.null(rmse)) return (NULL)
    result <- data.frame(RMSE_s = rmse, RMSE_ms = rmse * 1000)
  }

  stats_obj <- new_stats_dm(stat_df = result, type = "fit_stats")
  return(stats_obj)
}





# ACCESS FUNCTION FOR EACH STATISTIC --------------------------------------

#' Calculate Statistics for Model Prediction and/or Observed Data
#'
#' This function derives statistics that can be calculated for model
#' predictions and/or observed data. However, it does not calculate it, but
#' rather calls the respective backend functions.
#' Supported statistics currently include:
#' - Basic Summary Statistics (i.e., means and response percentages
#'   [dRiftDM::calc_basic_stats()])
#' - Conditional Accuracy Functions (CAFs; [dRiftDM::calc_cafs()])
#' - Quantiles ([dRiftDM::calc_quantiles()])
#' - Delta Functions ([dRiftDM::calc_delta_funs()]).
#' - Density Estimates ([dRiftDM::calc_dens()]).
#'
#' @param type character string, specifying the type of statistic to calculate.
#'   Available options are `"basic_stats"`, `"cafs"`, `"quantiles"`,
#'   `"delta_funs"`, and `"densities"`.
#' @param b_coding list for the boundary coding (see [dRiftDM::b_coding]).
#' @param conds character vector, specifying the conditions to include in
#' calculations (used for labeling and subsetting the model PDFs and the
#' observed data).
#' @param ... Additional parameters passed on to the specific statistic
#' calculation function (see Details).
#' @param scale_mass a single logical, only relevant for density estimation.
#' If `TRUE`, PDF masses are scaled proportional to the number of trials per
#' condition.
#'
#' @details
#'
#' When calling this function the arguments `all_rts_u`/`all_rts_l` and/or
#' `all_pdfs` must always be specified (see
#' [dRiftDM::re_evaluate_model], [dRiftDM::obs_data]). Otherwise, the backend
#' functions won't work properly. Further arguments are:
#'
#' - for CAFS: `n_bins` controls the number of bins, with a default of 5.
#' - for Quantiles and Delta Functions: `probs` controls the quantiles to
#' calculate. Default is `seq(0.1, 0.9, 0.1)`
#' (see [dRiftDM::drift_dm_default_probs()]).
#' - for basic summary satistics, Quantiles, and Delta Function:
#' `skip_if_contr_low` controls if quantiles and means are calculated for PDFs
#' with very small contribution (see also
#' [dRiftDM::drift_dm_skip_if_contr_low()]).
#' - for densities: `discr` controls the bin width for the observed data.
#' Default is 0.015 seconds
#'
#' This function gets called by [dRiftDM::calc_stats()]
#'
#'
#' @return A data frame with the calculated statistic across `conds`
#'  (ordered according to `Source`).
#'
#' @keywords internal
calc_stats_pred_obs <- function(type, b_coding, conds, ...,
                                scale_mass = FALSE) {
  dotdot <- list(...)

  if (!is.character(type) | length(type) != 1) {
    stop("type must be a single character string of length 1")
  }
  if (!is.logical(scale_mass) | length(scale_mass) != 1) {
    stop("scale_mass must be a single logical value of length 1")
  }

  # only these types are available for both observed and predicted data
  type <- match.arg(type, drift_dm_stats_types("sum_dist"))

  # iterate through the requested types and calculate the stats
  if (type == "basic_stats") {
    result <-
      lapply(conds, function(one_cond) {
        pdf_u <- dotdot$all_pdfs[[one_cond]]$pdf_u
        pdf_l <- dotdot$all_pdfs[[one_cond]]$pdf_l
        rts_u <- dotdot$all_rts_u[[one_cond]]
        rts_l <- dotdot$all_rts_l[[one_cond]]

        calc_basic_stats(
          pdf_u = pdf_u, pdf_l = pdf_l, rts_u = rts_u, rts_l = rts_l,
          one_cond = one_cond, b_coding = b_coding,
          t_vec = dotdot$t_vec, dt = dotdot$dt,
          skip_if_contr_low = dotdot$skip_if_contr_low
        )
      })
    result <- do.call("rbind", result)
  }
  if (type == "quantiles") {
    result <-
      lapply(conds, function(one_cond) {
        pdf_u <- dotdot$all_pdfs[[one_cond]]$pdf_u
        pdf_l <- dotdot$all_pdfs[[one_cond]]$pdf_l
        rts_u <- dotdot$all_rts_u[[one_cond]]
        rts_l <- dotdot$all_rts_l[[one_cond]]

        calc_quantiles(
          pdf_u = pdf_u, pdf_l = pdf_l, t_vec = dotdot$t_vec, dt = dotdot$dt,
          rts_u = rts_u, rts_l = rts_l,
          one_cond = one_cond, probs = dotdot$probs,
          b_coding = b_coding, skip_if_contr_low = dotdot$skip_if_contr_low
        )
      })
    result <- do.call("rbind", result)
  }
  if (type == "cafs") {
    result <-
      lapply(conds, function(one_cond) {
        pdf_u <- dotdot$all_pdfs[[one_cond]]$pdf_u
        pdf_l <- dotdot$all_pdfs[[one_cond]]$pdf_l
        rts_u <- dotdot$all_rts_u[[one_cond]]
        rts_l <- dotdot$all_rts_l[[one_cond]]

        calc_cafs(
          pdf_u = pdf_u, pdf_l = pdf_l, t_vec = dotdot$t_vec, rts_u = rts_u,
          rts_l = rts_l, one_cond = one_cond,
          n_bins = dotdot$n_bins, b_coding = b_coding
        )
      })
    result <- do.call("rbind", result)
  }
  if (type == "delta_funs") {
    interim <- calc_stats_pred_obs(
      type = "quantiles", b_coding = b_coding, conds = conds, ...
    )
    result <- calc_delta_funs(
      quantiles_dat = interim,
      minuends = dotdot$minuends,
      subtrahends = dotdot$subtrahends,
      dvs = dotdot$dvs, b_coding = b_coding
    )
  }
  if (type == "densities") {

    # determine the scaling factors
    scaling_factors = setNames(rep(1.0, length(conds)), conds)
    if (scale_mass) {
      n_per_cond <- sapply(conds, \(one_cond){
        rts_u <- dotdot$all_rts_u[[one_cond]]
        rts_l <- dotdot$all_rts_l[[one_cond]]
        return(length(rts_u) + length(rts_l))
      })
      # if there are no trials, fall back to scaling of 1.0
      if (any(n_per_cond == 0)) {
        scaling_factors = setNames(rep(1.0, length(conds)), conds)
      } else {
        scaling_factors <- n_per_cond / (sum(n_per_cond) / length(n_per_cond))
      }
    }

    # then iterate again over all conditions
    result <-
      lapply(conds, function(one_cond) {
        pdf_u <- dotdot$all_pdfs[[one_cond]]$pdf_u
        pdf_l <- dotdot$all_pdfs[[one_cond]]$pdf_l
        rts_u <- dotdot$all_rts_u[[one_cond]]
        rts_l <- dotdot$all_rts_l[[one_cond]]
        scaling_factor <- scaling_factors[[one_cond]]

        calc_dens(
          pdf_u = pdf_u, pdf_l = pdf_l, t_vec = dotdot$t_vec,
          t_max = dotdot$t_max, discr = dotdot$discr, rts_u = rts_u,
          rts_l = rts_l, one_cond = one_cond, b_coding = b_coding,
          scaling_factor = scaling_factor
        )
      })
    result <- do.call("rbind", result)
  }

  result <- result[order(result$Source), ]
  rownames(result) <- NULL

  return(result)
}




# METHODS FOR drift_dm and data.frame OBJECTS TO CALCULATE STATS  ----------


#' Calculate Statistics
#'
#' @description
#'
#' `calc_stats` provides an interface for calculating statistics/metrics on
#' model predictions and/or observed data. Supported statistics include
#' basic statistics on mean and standard deviation, Conditional Accuracy
#' Functions (CAFs), Quantiles, Delta Functions, and fit statistics. Results can
#' be aggregated across individuals.
#'
#' @param object an object for which statistics are calculated. This can be a
#' [data.frame] of observed data, a [dRiftDM::drift_dm] object, a
#' `fits_ids_dm` object, or a `fits_agg_dm` object (see
#'  [dRiftDM::estimate_dm()]).
#' @param type a character vector, specifying the statistics to calculate.
#' Supported values include `"basic_stats"`, `"cafs"`, `"quantiles"`,
#' `"delta_funs"`, and `"fit_stats"`.
#' @param ... additional arguments passed to the respective method and the
#' underlying calculation functions (see Details for mandatory arguments).
#' @param conds optional character vector specifying conditions to include.
#' Conditions must match those found in the `object`.
#' @param resample logical. If `TRUE`, then data is (re-)sampled to create
#' an uncertainty estimate for the requested statistic. See Details for more
#' information). Default is `FALSE`.
#' @param split_by_ID logical. If `TRUE`, statistics are calculated separately
#' for each individual ID in `object` (when `object` is a [data.frame]). Default
#' is `TRUE`.
#' @param b_coding a list for boundary coding (see [dRiftDM::b_coding]). Only
#' relevant when `object` is a [data.frame]. For other `object` types, the
#' `b_coding` of the `object` is used.
#' @param average logical. If `TRUE`, averages the statistics across individuals
#' where applicable. Default for `calc_stats.data.frame` is `FALSE`. Default
#' for `calc_stats.fits_agg_dm` is `TRUE`.
#' @param progress integer, indicating if information about the progress
#'  should be displayed. 0 -> no information, 1 -> a progress bar. Default is 1.
#' @param round_digits integer, controls the number of digits shown.
#'  Default is 3.
#' @param print_rows integer, controls the number of rows shown.
#' @param some logical. If `TRUE`, a subset of randomly sampled rows is shown.
#' @param show_header logical. If `TRUE`, a header specifying the type of
#'  statistic will be displayed.
#' @param show_note logical. If `TRUE`, a footnote  is displayed indicating
#' that the underlying [data.frame] can be accessed as usual.
#' @param x an object of type `stats_dm` or `stats_dm_list`, as returned by
#' the function `calc_stats()`.
#'
#' @details
#' `calc_stats` is a generic function to handle the calculation of different
#' statistics/metrics for the supported object types. Per default, it returns
#' the requested statistics/metrics.
#'
#' ## List of Supported Statistics
#'
#' **Basic Statistics**
#'
#' With "basic statistics", we refer to a summary of the mean and standard
#' deviation of response times, including a proportion of response choices.
#'
#'
#' **Conditional Accuracy Function (CAFs)**
#'
#' CAFs are a way to quantify response accuracy against speed. To calculate
#' CAFs, RTs (whether correct or incorrect) are first binned and then the
#' percent correct responses per bin is calculated.
#'
#' When calculating model-based CAFs, a joint CDF combining both the pdf
#' of correct and incorrect responses is calculated. Afterwards, this CDF
#' is separated into even-spaced segments and the contribution of
#' the pdf associated with a correct response relative to the joint CDF is
#' calculated.
#'
#' The number of bins can be controlled by passing the argument `n_bins`.
#' The default is 5.
#'
#' **Quantiles**
#'
#'  For observed response times, the function [stats::quantile] is used with
#'  default settings.
#'
#'  Which quantiles are calcuated can be controlled by providing the
#'  probabilites, `probs`, with values in \eqn{[0, 1]}. Default is
#'  `seq(0.1, 0.9, 0.1)`.
#'
#' **Delta Functions**
#'
#'  Delta functions calculate the difference between quantiles
#'  of two conditions against their mean:
#'  - \eqn{Delta_i = Q_{i,j} - Q_{i,k}}
#'  - \eqn{Avg_i = 0.5 \cdot Q_{i,j} + 0.5 \cdot Q_{i,k}}
#'
#'  With i indicating a quantile, and j and k two conditions.
#'
#'  To calculate delta functions, users have to specify:
#'  - `minuends`: character vector, specifying condition(s) j. Must be in
#'    `conds(drift_dm_obj)`.
#'  - `subtrahends`: character vector, specifying condition(s) k. Must be in
#'    `conds(drift_dm_obj)`
#'  - `dvs`: character, indicating which quantile columns to use.
#'     Default is "Quant_<u_label>". If multiple dvs are provided,
#'     then minuends and subtrahends must have the same length,
#'     and matching occurs pairwise. In this case, if only one
#'     minuend/subtrahend is specified, minuend and subtrahend are recycled to
#'     the necessary length.
#'
#'
#' **Fit Statistics**
#'
#' Calculates the Akaike and Bayesian Information Criteria (AIC and BIC). Users
#' can provide a `k` argument to penalize the AIC statistic (see [stats::AIC]
#' and [dRiftDM::AIC.fits_ids_dm])
#'
#'
#' ## Resampling
#'
#' When users set `resampling = TRUE`, the statistics TODO
#'
#'
#' @returns
#' If `type` is a single character string, then a subclass of [data.frame] is
#' returned, containing the respective statistic. Objects of type `sum_dist`
#' will have an additional attribute storing the boundary encoding (see also
#' [dRiftDM::b_coding]). The reason for returning subclasses of [data.frame] is
#' to provide custom `plot()` methods (e.g., [dRiftDM::plot.cafs]). To get rid
#' of the subclass label and additional attributes (i.e., to get just the plain
#' underlying [data.frame], users can use [dRiftDM::unpack_obj()]).
#'
#' If `type` contains multiple character strings (i.e., is a character vector) a
#' subclass of [list] with the calculated statistics is returned. The list will
#' be of type `stats_dm_list` (to easily create multiple panels using the
#' respective [dRiftDM::plot.stats_dm_list()] method).
#'
#' The print methods `print.stats_dm()` and `print.stats_dm_list()` each
#' invisibly return the supplied object `x`.
#'
#' @note
#'
#' When a model's predicted density function integrates to a value of less than
#' [dRiftDM::drift_dm_skip_if_contr_low()], means and quantiles return the
#' values `NA`. Users can alter this by explicitly passing the argument
#' `skip_if_contr_low` when calling `calc_stats()` (e.g.,
#' `calc_stats(..., skip_if_contr_low = -Inf)`)
#'
#'
#' @examples
#' # Example 1: Calculate CAFs and Quantiles from a model ---------------------
#' # get a model for demonstration purpose
#' a_model <- ssp_dm(dx = .0025, dt = .0025, t_max = 2)
#' # and then calculate cafs and quantiles
#' some_stats <- calc_stats(a_model, type = c("cafs", "quantiles"))
#' print(some_stats)
#'
#' # Example 2: Calculate a Delta Function from a data.frame ------------------
#' # get a data set for demonstration purpose
#' some_data <- ulrich_simon_data
#' conds(some_data) # relevant for minuends and subtrahends
#' some_stats <- calc_stats(
#'   a_model,
#'   type = "delta_funs",
#'   minuends = "incomp",
#'   subtrahends = "comp"
#' )
#' print(some_stats, print_rows = 5)
#'
#'
#' # Example 3: Calculate Quantiles from a fits_ids_dm object -----------------
#' # get an auxiliary fits_ids_dm object
#' all_fits <- get_example_fits_ids()
#' some_stats <- calc_stats(all_fits, type = "quantiles")
#' print(some_stats, print_rows = 5) # note the ID column
#'
#' # one can also request that the statistics are averaged across individuals
#' print(
#'   calc_stats(all_fits, type = "quantiles", average = TRUE)
#' )
#'
#' @export
calc_stats <- function(object, type, ...) {

  # to clean up any temporary options, exploited during the calculation of
  # the statistics
  withr::defer(stats.options(NULL))


  if (length(type) > 1) {
    all_stats <- sapply(type, function(one_type) {
      calc_stats(object = object, type = one_type, ...)
    }, simplify = FALSE, USE.NAMES = TRUE)

    class(all_stats) <- c("stats_dm_list")
    return(all_stats)
  }

  UseMethod("calc_stats")
}


#' @rdname calc_stats
#' @export
calc_stats.data.frame <- function(object, type, ..., conds = NULL,
                                  resample = FALSE, progress = 1,
                                  level = "individual", b_coding = NULL) {

  obs_data <- object
  dots = list(...)


  # input checks
  type <- match.arg(type, drift_dm_stats_types("sum_dist"))
  if (!is.logical(resample) || length(resample) != 1) {
    stop("resample must be a single logical")
  }
  if (!(progress %in% c(0, 1))) {
    stop("progress must be 0 or 1")
  }
  level = match.arg(level, c("individual", "group"))

  # deprecation warning about split_by_ID and average
  if (!is.null(dots$split_by_ID)) {
    lifecycle::deprecate_warn(
      when = "0.3.0", what = "calc_stats.data.frame(split_by_ID = )",
      with = "calc_stats.data.frame(level =)"
    )
    if (dots$split_by_ID) {
      level = "individual"
    } else {
      level = "group"
    }
  }

  if (!is.null(dots$average)) {
    lifecycle::deprecate_warn(
      when = "0.3.0", what = "calc_stats.data.frame(average = )",
      with = "calc_stats.data.frame(level =)"
    )
    if (dots$average) {
      level = "group"
    } else {
      level = "individual"
    }
  }

  # get b_coding and check the data
  if (is.null(b_coding)) {
    b_coding <- drift_dm_default_b_coding()
  }

  obs_data <- check_reduce_raw_data(
    obs_data = obs_data,
    b_coding_column = b_coding$column,
    u_value = b_coding$u_name_value,
    l_value = b_coding$l_name_value
  )

  # get the condition labels
  data_conds <- unique(obs_data$Cond)
  if (is.null(conds)) {
    conds <- data_conds
  } else {
    conds <- match.arg(arg = conds, choices = data_conds, several.ok = TRUE)
  }


  #  calculate for each ID (if possible)
  if (level == "individual" && ("ID" %in% colnames(obs_data))) {


    # temporarily set t_max as an option, if it was not specified,
    # this ensures a consistent size of outputs (like in density)
    stats.options(t_max = max(obs_data$RT))


    # split data up
    list_obs_data <- split(x = obs_data, f = obs_data$ID)

    # create a progress bar
    if (progress == 1) {
      n_iter <- length(list_obs_data)
      pb <- progress::progress_bar$new(
        format = "calculating [:bar] :percent; done in: :eta",
        total = n_iter, clear = FALSE, width = 60
      )
      pb$tick(0)
    }

    # iterate over the list of individual data
    all_results <- lapply(names(list_obs_data), function(id) {
      sub_dat = list_obs_data[[id]]
      sub_dat = sub_dat[setdiff(names(sub_dat), "ID")]
      stat <- calc_stats.data.frame(
        object = sub_dat, type = type, ..., conds = conds, resample = resample,
        progress = 0, level = "individual", b_coding = b_coding
      )
      stat_id <- cbind(ID = try_cast_integer(id), stat)
      stat_id <- copy_class_attributes(old = stat, new = stat_id)
      if (progress == 1) pb$tick()
      return(stat_id)
    })
    results <- do.call("rbind", all_results) # preserves class and attributes
    results <- results[order(results$ID), ]
    rownames(results) <- NULL
    return(results)

  }



  # if level == "group", try to return at the group-level
  # -> i.e., average across ID column or bootstrap entire sample
  if (level == "group") {


    if (!("ID" %in% colnames(obs_data))) {
      stop("Statistics at the group level can only be calculated if the data ",
           "set contains an 'ID' column")
    }

    # if resample is requested, call the underlying bootstrap function and return
    if (resample) {
      result <- stats_resample_dm(
        object = obs_data, type = type, conds = conds, ..., b_coding = b_coding,
        progress = progress, level = level
      )
      return(result)
    }

    # otherwise calculate the statistics for each individual and then average
    # across it
    result <- calc_stats.data.frame(
      object = obs_data, type = type, ..., conds = conds, resample = FALSE,
      progress = progress, level = "individual", b_coding = b_coding
    )
    return(aggregate_stats(result))
  }

  # finally, if individual and/or no ID column, assume obs_data is from one
  # individual
  if (resample) {
    result <- stats_resample_dm(
      object = obs_data, type = type, conds = conds, ..., b_coding = b_coding,
      progress = progress, level = level
    )
    return(result)
  }

  rts <- obs_data_to_rt_lists(obs_data = obs_data, b_coding = b_coding)

  # call the internal function
  result <- calc_stats_pred_obs(
    type = type, b_coding = b_coding,
    conds = conds,
    all_rts_u = rts$rts_u,
    all_rts_l = rts$rts_l, ...
  )

  return(result)
}


#' @rdname calc_stats
#' @export
calc_stats.drift_dm <- function(object, type, ..., conds = NULL,
                                resample = FALSE) {
  drift_dm_obj <- object
  type <- match.arg(type, drift_dm_stats_types("drift_dm"))
  dots = list(...)



  # if fit_stats requested, return it
  if (type == "fit_stats") {
    result <- calc_fit_stats(drift_dm_obj, ...)
    return(result)
  }

  # otherwise, continue with summary stats
  # get the conds
  model_conds <- conds(drift_dm_obj)
  if (is.null(conds)) {
    conds <- model_conds
  } else {
    conds <- match.arg(arg = conds, choices = model_conds, several.ok = TRUE)
  }

  # get b_coding
  b_coding <- attr(drift_dm_obj, "b_coding")

  # get time space
  t_max <- drift_dm_obj$prms_solve[["t_max"]]
  nt <- drift_dm_obj$prms_solve[["nt"]]
  t_vec <- seq(0, t_max, length.out = nt + 1)

  # get pdfs for quick reference
  if (is.null(drift_dm_obj$pdfs)) {
    drift_dm_obj <- re_evaluate_model(
      drift_dm_obj = drift_dm_obj,
      eval_model = TRUE
    )
  }

  # temporarily set t_max as an option, if it was not specified,
  # this ensures a consistent size of outputs (like in density)
  stats.options(t_max = t_max)


  # if resample is requested, call the underlying bootstrap function and return
  # -> stats_resample_dm returns a stats_dm object of type sum_dist, with
  # the additional column "Estimate" that codes a lower and upper boundary, as
  # well as the original statistic
  if (resample) {
    out <- stats_resample_dm(
      object = drift_dm_obj, conds = conds, type = type, b_coding = b_coding,
      ...
    )
    return(out)
  }


  # otherwise continue with a call to calc_stats_pred_obs
  # ... extract pdfs and check if at least 1% of the PDFs is missing
  all_pdfs <- drift_dm_obj$pdfs
  dt <- drift_dm_obj$prms_solve[["dt"]]
  check_loss <- sapply(all_pdfs, function(one_set_pdfs) {
    sum_both <- sum(one_set_pdfs$pdf_u) * dt + sum(one_set_pdfs$pdf_l) * dt
    return(sum_both < .99)
  }, simplify = TRUE, USE.NAMES = TRUE)
  if (any(check_loss)) {
    warning(
      "calc_stats called with missing probability mass for some ",
      "conditions (likely occured after truncating pdfs to the ",
      "time space). Some statistics scale the pdfs! ",
      "Interprete 'quantiles' etc. accordingly (or increase t_max)."
    )
  }

  # and call the underlying internal function calc_stats_pred_obs
  result <- calc_stats_pred_obs(
    type = type, b_coding = b_coding,
    conds = conds, all_pdfs = all_pdfs,
    all_rts_u = drift_dm_obj$obs_data$rts_u,
    all_rts_l = drift_dm_obj$obs_data$rts_l,
    t_vec = t_vec, dt = dt, ...
  )

  return(result)
}


#' @rdname calc_stats
#' @export
calc_stats.fits_ids_dm <- function(object, type, ..., conds = NULL,
                                   resample = FALSE, progress = 1,
                                   level = "individual") {
  fits_ids <- object
  dots <- list(...)

  # input checks
  type <- match.arg(type, drift_dm_stats_types("sum_dist"))

  model_conds <- conds(fits_ids)
  if (is.null(conds)) {
    conds <- model_conds
  } else {
    conds <- match.arg(arg = conds, choices = model_conds, several.ok = TRUE)
  }

  if (!(progress %in% c(0, 1))) {
    stop("progress must be 0 or 1")
  }
  level = match.arg(level, c("individual", "group"))


  # deprecation warning about average
  if (!is.null(dots$average)) {
    lifecycle::deprecate_warn(
      when = "0.3.0", what = "calc_stats.data.frame(average = )",
      with = "calc_stats.data.frame(level =)"
    )
    if (dots$average) {
      level = "group"
    } else {
      level = "individual"
    }
  }

  # temporarily set t_max as an option, if it was not specified,
  # this ensures a consistent size of outputs (like in density)
  stats.options(t_max = prms_solve(fits_ids)[["t_max"]])


  # if resample at the group level (i.e., resample individuals),
  # call the underlying method
  if (resample && level == "group") {
    result <- stats_resample_dm(
      object = fits_ids, conds = conds, type = type,
      b_coding = b_coding(fits_ids), ..., progress = progress, level = level
    )
    return(result)
  }

  # otherwise get the data of each individual
  # create a progress bar
  if (progress >= 1) {
    n_iter <- length(fits_ids$all_fits)
    pb <- progress::progress_bar$new(
      format = "calculating [:bar] :percent; done in: :eta",
      total = n_iter, clear = FALSE, width = 60
    )
    pb$tick(0)
  }

  # call statistic across individuals
  all_results <- lapply(names(fits_ids$all_fits), function(id) {
    stat <- calc_stats.drift_dm(
      object = fits_ids$all_fits[[id]], type = type, ..., conds = conds,
      resample = resample
    )
    stat_id <- cbind(ID = try_cast_integer(id), stat)
    stat_id <- copy_class_attributes(old = stat, new = stat_id)
    if (progress == 1) pb$tick()
    return(stat_id)
  })
  results <- do.call("rbind", all_results) # preserves class and attributes
  results <- results[order(results$ID), ]
  rownames(results) <- NULL

  # if group-level is requested, average across IDS
  if (level == "group") {
    results <- aggregate_stats(results)
  }

  return(results)
}


#' @rdname calc_stats
#' @export
calc_stats.fits_agg_dm <- function(object, type, ..., conds = NULL,
                                   resample = FALSE, progress = 1,
                                   level = "group", messaging = TRUE) {

  fits_agg <- object
  dots <- list(...)
  b_coding = b_coding(fits_agg)
  obs_data = obs_data(fits_agg)

  # input checks
  model_conds <- conds(fits_agg)
  if (is.null(conds)) {
    conds <- model_conds
  } else {
    conds <- match.arg(arg = conds, choices = model_conds, several.ok = TRUE)
  }

  if (!(progress %in% c(0, 1))) {
    stop("progress must be 0 or 1")
  }
  level = match.arg(level, c("individual", "group"))


  # deprecation warning about average
  if (!is.null(dots$average)) {
    lifecycle::deprecate_warn(
      when = "0.3.0", what = "calc_stats.data.frame(average = )",
      with = "calc_stats.data.frame(level =)"
    )
    if (dots$average) {
      level = "group"
    } else {
      level = "individual"
    }
  }

  # temporarily set t_max as an option, if it was not specified,
  # this ensures a consistent size of outputs (like in density)
  stats.options(t_max = prms_solve(fits_agg)[["t_max"]])


  # if resample is requested, handle the observed and predicted data
  # separately
  if (resample) {

    # observed resample results (via calc_stats to provide sampling for each
    # individual)
    results_obs <- calc_stats.data.frame(
      object = obs_data, type = type, ..., conds = conds, resample = TRUE,
      progress = progress, level = level, b_coding = b_coding
    )

    # resample predicted
    if (messaging) {
      message(
        "Generating simulations under the model using the average trial ",
        "number across participants. Other options are not available for ",
        "objects of type 'fits_agg_dm'."
      )
    }
    n_sim = summary(fits_agg)$obs_data$avg_trials
    results_pred <- stats_resample_dm.drift_dm(
      object = fits_agg$drift_dm_obj, conds = conds, type = type,
      b_coding = b_coding, ..., n_sim = n_sim
    )
    if (level == "individual") {
      results_pred = cbind(ID = NA, results_pred) # to ensure rbind works
    }

    # row bind and return
    return(rbind(results_obs, results_pred))
  }


  # if no resampling required, then get the standard model stats
  stats_pred = calc_stats.drift_dm(
    object = fits_agg$drift_dm_obj, type = type, ..., conds = conds,
    resample = FALSE
  )
  # add a fake ID column if individual was requested and observed data exists
  if (level == "individual") {
    stats_pred = cbind(ID = NA, stats_pred)
  }

  # if statistics are requested that can not be used for observed data stop here
  check = setdiff(drift_dm_stats_types(), drift_dm_stats_types("data.frame"))
  if (type %in% check) {
    return(stats_pred)
  }

  # get the observed stats
  stats_obs = calc_stats.data.frame(
    object = obs_data, type = type, ..., conds = conds, resample = FALSE,
    progress = progress, level = level, b_coding = b_coding
  )

  # combine and pass back
  results = rbind(stats_obs, stats_pred)

  return(results)
}



# RESAMPLE -----------------------------------------------------------------

#' Resample Observed and Predicted Statistics for Interval Estimation
#'
#' Internal methods to generate bootstrap-like intervals for
#' descriptive statistics derived from either observed data or model
#' predictions. These methods support both `drift_dm` objects and
#' data.frames containing a single participant's observed data.
#'
#'
#' @param object a `drift_dm` object (for model-based resampling) or a
#'   [data.frame] with observed data for a single participant.
#'   `drift_dm_stats_types("sum_dists")`, such as `"quantiles"` or `"cafs"`.
#' @param conds a character vector indicating the condition(s) for which the
#'   statistics should be resampled.
#' @param type a character string, specifying the `type` of statistic to
#'   calculate
#' @param R an integer, number of replications (default is 100).
#' @param interval_level a numeric between 0 and 1, controlling the width of the
#' interval (default is 0.95).
#' @param b_coding a list, specifying the boundary coding, required
#'   when calculating the statistics.
#' @param n_sim an optional vector, providing the trial numbers for simulating
#' synthetic data under the model. Only relevant when no observed data is
#' attached to the model via the `obs_data` entry of the model.
#' @param progress an integer, specifying if a progress bar shall be displayed
#'  (`1`) or not (`0`).
#' @param level a character string, specifying at which level resampling shall
#'  take place. `"individual"` will lead to resampling of an individual's data.
#'  `"group"` will lead to resampling of the entire participant.
#'
#' @param ... additional arguments passed to
#' [dRiftDM::stats_resample_wrapper()] and [dRiftDM::simulate_data()]. Must
#' contain `type` and `b_coding`
#'
#' @details
#'
#' The `stats_resample_dm()` generic dispatches to class-specific methods.
#' For `drift_dm` objects, it generates synthetic data sets under the model.
#' For raw data, it resamples observations with replacement (i.e, it performs
#' a bootstrap). In both cases, statistics from the resampled/generated data are
#' used to compute intervals for the requested statistic.
#'
#' Resampling is done for each condition separately.
#'
#' The function `stats_resample_dm()` is called within [dRiftDM::calc_stats()].
#'
#' @return
#' A `stats_dm` object with added column `Estimate` indicating whether the row
#' represents the lower interval bound, the original value (`"orig"`), or the
#' upper interval bound. The interval level can be controlled via the
#' `interval_level` argument.
#'
#' @keywords internal
#' @name stats_resample_dm
stats_resample_dm <- function(object, conds, type, b_coding, ..., R,
                              interval_level) {
  UseMethod("stats_resample_dm")

}

#' @rdname stats_resample_dm
#' @export
stats_resample_dm.drift_dm <- function(object, conds, type, b_coding, ...,
                                       R = 100, interval_level = 0.95,
                                       n_sim = NULL) {

  drift_dm_obj <- object
  dotdot <- list(...)


  obs_data <- obs_data(drift_dm_obj, messaging = FALSE)
  if (is.null(obs_data)) {
    stopifnot(!is.null(n_sim))
  }

  # now get the results for the observed data (if possible)
  result_obs <- NULL
  obs_data <- obs_data[obs_data$Cond %in% conds,]
  if (!is.null(obs_data)) {
    result_obs <- stats_resample_dm.data.frame(
      object = obs_data, conds = conds, type = type, b_coding = b_coding,
      ..., R = R, interval_level = interval_level, progress = 0,
      level = "individual"
    )
  }

  # now get the results for the predicted data
  obs_data(drift_dm_obj) <- NULL

  # re_evaluate to get the pdfs if necessary
  if (is.null(drift_dm_obj$pdfs)) {
    drift_dm_obj <- re_evaluate_model(
      drift_dm_obj = drift_dm_obj,
      eval_model = TRUE
    )
  }

  # get the synthetic data (in a format suitable for stats_resample_wrapper)
  if (is.null(n_sim)) {
    n_sim = sapply(conds, \(one_cond) nrow(obs_data[obs_data$Cond == one_cond,]))
  } else {
    n_sim = n_sim[conds]
  }

  get_synth_data <- function(){
    one_synth_data = simulate_data.drift_dm(
      object = drift_dm_obj, n = n_sim, conds = conds,
      round_to = dotdot$round_to
    )
    split(one_synth_data, one_synth_data$Cond)
  }

  synth_data = replicate(n = R,expr = get_synth_data(), simplify = FALSE)

  # calculate the statistics for each synthetic data set
  resample_list = do_resampling(
    lapply(synth_data, function(one_dat_split){
      stats_resample_wrapper(
        one_dat_split, type = type, b_coding = b_coding, ...
      )
    })
  )


  # get the original statistic
  original = calc_stats.drift_dm(
    object = drift_dm_obj, type = type, ..., conds = conds,
    resample = FALSE
  )
  original = original[original$Source == "pred",]
  rownames(original) <- NULL

  # get the borders and Estimate column
  result_pred = resample_assemble(
    resample_list, level = interval_level, original = original
  )

  # finally, bind everything together and pass back
  result = rbind(result_obs, result_pred)
  return(result)
}


#' @rdname stats_resample_dm
#' @export
stats_resample_dm.data.frame <- function(object, conds, type, b_coding, ...,
                                         R = 100, interval_level = 0.95,
                                         progress = 0, level) {

  obs_data = object



  # level == "individual" treat the observed data as if it is from a single
  # participant
  if (level == "individual") {
    stopifnot(!("ID" %in% names(obs_data))) # mustn't have an ID column

    # reduce and split the data by condition
    obs_data = obs_data[obs_data$Cond %in% conds,]
    obs_data_split = split(obs_data, obs_data$Cond)


    # helper function: creates idxs for each condition, returns a list of indices
    # for each condition
    get_idxs = function() {
      n_obs = lapply(obs_data_split, nrow)
      lapply(n_obs, \(x) sample(seq_len(x), size = x, replace = TRUE))
    }

    idx_list = replicate(n = R, expr = get_idxs(), simplify = FALSE)

    # call the stats_resample_wrapper function for each set of indices
    resample_list = do_resampling(
      lapply(idx_list, function(one_set_idxs){
        stats_resample_wrapper(
          obs_data_split, one_set_idxs, type = type, b_coding = b_coding, ...
        )
      })
    )

    # get the original statistic
    original = stats_resample_wrapper(
      obs_data_split, type = type, b_coding = b_coding, ...
    )


  } else if (level == "group") {
    stopifnot(("ID" %in% names(obs_data))) # must have an ID column



    # reduce to relevant conditions
    obs_data = obs_data[obs_data$Cond %in% conds,]

    # get the original statistic
    original = calc_stats.data.frame(
      object = obs_data, type = type, ..., conds = conds, resample = FALSE,
      progress = 0, level = level, b_coding = b_coding
    )

    # split by subject and then by cond
    obs_data_split = split(obs_data, obs_data$ID)
    obs_data_split = lapply(obs_data_split, \(x) split(x, x$Cond))


    # create indices of participants across replications
    idxs = seq_along(obs_data_split)

    idx_list = replicate(
      n = R, expr = sample(x = idxs, size = max(idxs), replace = TRUE),
      simplify = FALSE
    )

    # progress bar output
    if (progress == 1) {
      n_iter <- length(idx_list)
      pb <- progress::progress_bar$new(
        format = "calculating observed [:bar] :percent; done in: :eta",
        total = n_iter, clear = FALSE, width = 60
      )
      pb$tick(0)
    }

    # then shuffle the participants and calculate the average statistics for
    # each shuffle
    resample_list = do_resampling(
      lapply(idx_list, function(one_set_idxs){
        unique_idxs <- unique(one_set_idxs)

        # Call only once per unique index
        cached_stats <- lapply(unique_idxs, function(i) {
          stat <- stats_resample_wrapper(
            obs_data_split[[i]], type = type, b_coding = b_coding, ...
          )
          return(stat)
        })

        # match to replicate and then average
        idx_map <- match(one_set_idxs, unique_idxs)  # positions in cached_stats
        all_stats <- do.call(rbind, cached_stats[idx_map])

        # create some fake IDs to exploit the aggregate_stats function
        how_often = nrow(all_stats)/max(idxs)
        stopifnot(how_often %% 1 == 0)
        all_stats = cbind(ID = rep(idxs, each = how_often), all_stats)

        # aggregate and pass back
        all_stats <- copy_class_attributes(old = original, new = all_stats)
        all_stats <- aggregate_stats(all_stats)
        if (progress == 1) pb$tick()
        return(all_stats)
      })
    )

  }

  # get the level borders and Estimate column
  result = resample_assemble(
    resample_list, level = interval_level, original = original
  )

  return(result)
}


#' @rdname stats_resample_dm
#' @export
stats_resample_dm.fits_ids_dm <- function(object, conds, type, b_coding, ...,
                                          R = 100, interval_level = 0.95,
                                          progress = 0, level) {

  fits_ids = object
  # resampling at the individual level is solved by calling
  # calc_stats.drift_dm repeatedly from calc_stats.fits_ids_dm
  stopifnot(level == "group")



  # now get the results for the observed data
  obs_data = fits_ids$drift_dm_fit_info$obs_data_ids
  result_obs <- stats_resample_dm.data.frame(
    object = obs_data, conds = conds, type = type, b_coding = b_coding,
    ..., R = R, interval_level = interval_level, progress = progress,
    level = level
  )


  # now get the results for the predicted data
  # detach observed data to avoid calculating this as well
  fits_ids$all_fits = lapply(fits_ids$all_fits, \(x) {obs_data(x) <- NULL; x})
  all_fits = fits_ids$all_fits

  # get the original statistic
  original = calc_stats.fits_ids_dm(
    object = fits_ids, type = type, ..., conds = conds, resample = FALSE,
    progress = 0, level = level
  )

  # create indices of participants across replications
  idxs = seq_along(all_fits)
  idx_list = replicate(
    n = R, expr = sample(x = idxs, size = max(idxs), replace = TRUE),
    simplify = FALSE
  )

  # progress bar output
  if (progress == 1) {
    n_iter <- length(idx_list)
    pb <- progress::progress_bar$new(
      format = "calculating predicted [:bar] :percent; done in: :eta",
      total = n_iter, clear = FALSE, width = 60
    )
    pb$tick(0)
  }


  # then shuffle the participants and calculate the average statistics for
  # each shuffle
  resample_list = do_resampling(
    lapply(idx_list, function(one_set_idxs){
      unique_idxs <- unique(one_set_idxs)

      # Call only once per unique index
      cached_stats <- lapply(unique_idxs, function(i) {
        stat <- calc_stats.drift_dm(
          all_fits[[i]], type = type, ..., conds = conds, resample = FALSE
        )
        stat <- cbind(ID = i, stat)
        return(stat)
      })

      # match to replicate and then average
      idx_map <- match(one_set_idxs, unique_idxs)  # positions in cached_stats
      all_stats <- do.call(rbind, cached_stats[idx_map])

      # create some fake IDs to exploit the aggregate_stats function
      how_often = nrow(all_stats)/max(idxs)
      stopifnot(how_often %% 1 == 0)
      all_stats = cbind(ID = rep(idxs, each = how_often), all_stats)

      # aggregate and pass back
      all_stats <- copy_class_attributes(old = original, new = all_stats)
      all_stats <- aggregate_stats(all_stats)
      if (progress == 1) pb$tick()
      return(all_stats)
    })
  )

  # get the level borders and Estimate column
  result_pred = resample_assemble(
    resample_list, level = interval_level, original = original
  )


  result = rbind(result_obs, result_pred)
  return(result)
}


# RESAMPLE HELPERS --------------------------------------------------------

#' Temporarily suppress new stats generation during resampling
#'
#' Internal helper that sets the `skip_new_stats_dm` and
#' `skip_validate_stats_dm` options to `TRUE` before
#' evaluating an expression, and resets them to `NULL` afterward (see also
#' [dRiftDM::stats.options()]). Intended to
#' prevent the (unncessary) creation/checking of stats_dm objects during
#' resampling.
#'
#' @param x An expression to evaluate.
#' @return The result of evaluated `x`.
#'
#' @keywords internal
do_resampling = function(x) {
  stats.options(skip_new_stats_dm = TRUE)
  stats.options(skip_validate_stats_dm = TRUE)
  withr::defer(expr = {
    stats.options(skip_new_stats_dm = NULL)
    stats.options(skip_validate_stats_dm = NULL)
  })
  force(x)
}



#' Internal Helpers for Resampling of Summary Statistics
#'
#' These functions support the construction of intervals for
#' descriptive statistics computed from observed or simulated
#' data. They are used internally by [dRiftDM::stats_resample_dm()] methods.
#'
#'
#' @param obs_data_split a named list of [data.frame]s, containing a single set
#' of observed data, splitted by condition
#' @param one_set_idxs a named list of numeric vectors. Each entry contains
#' indices to shuffle the [data.frame]s in `obs_data_split`. Default `NULL`
#' keeps `obs_data_split` as is.
#' @param type a character, passed to [dRiftDM::calc_stats_pred_obs()].
#' @param b_coding a list with boundary coding information,
#' , required to wrangle rts to match with [dRiftDM::calc_stats_pred_obs()].
#' @param ... additional arguments passed to [dRiftDM::calc_stats_pred_obs()].
#' @param resample_list a list of statistics returned by calls to
#'   `stats_resample_wrapper()`.
#' @param level a numeric between 0 and 1, controlling the width of the interval.
#' @param original a `stats_dm` object representing the statistic computed
#'   from the original data set or model prediction.
#'
#'
#' @details
#' `stats_resample_wrapper()` wraps a call to `calc_stats_pred_obs()` for use
#' in resampling.
#'
#' `resample_assemble()` takes a list of resampled statistics and the
#' original statistic, and computes lower and upper bounds based
#' on the requested level. It returns a `stats_dm` object with an
#' added `Estimate` column.
#'
#'
#' @return
#' - `stats_resample_wrapper()` returns a single `stats_dm` object for one
#'   sample.
#' - `resample_assemble()` returns a `stats_dm` object containing the lower
#'   and upper interval bounds along with the original estimate.
#'
#' @keywords internal
#' @name resample_helpers
stats_resample_wrapper <- function(obs_data_split, one_set_idxs = NULL, type,
                                   b_coding, ...) {

  if (!(type %in% drift_dm_stats_types("sum_dist"))) {
    stop("the requested statistic type ('", type, "') can't be resampled")
  }

  conds = names(obs_data_split)
  stopifnot(!is.null(conds))


  # shuffle within each condition (if requested)
  if (!is.null(one_set_idxs)) {
    stopifnot(setequal(names(one_set_idxs), conds))
    obs_data_split = sapply(conds, \(one_cond){
      obs_data_split[[one_cond]][one_set_idxs[[one_cond]],]
    }, simplify = FALSE, USE.NAMES = TRUE)
  }

  # get the rts lists (identical to obs_data_to_rt_lists)
  b_column <- b_coding$column
  u_name_value <- b_coding$u_name_value
  l_name_value <- b_coding$l_name_value

  rts_u <- list()
  rts_l <- list()
  for (one_cond in conds) {
    sub_dat <- obs_data_split[[one_cond]]
    rts_u[[one_cond]] <- sub_dat$RT[sub_dat[[b_column]] == u_name_value]
    rts_l[[one_cond]] <- sub_dat$RT[sub_dat[[b_column]] == l_name_value]
  }


  # call the statistic function
  out <- calc_stats_pred_obs(
    type = type, b_coding = b_coding,
    conds = conds,
    all_rts_u = rts_u,
    all_rts_l = rts_l, ...
  )
  return(out)
}

#' @rdname resample_helpers
resample_assemble <- function(resample_list, level, original) {

  stopifnot(!("ID" %in% names(original)))

  # figure out the column names etc.
  stat_type = class(original)[1]
  first_boot = resample_list[[1]]
  all_cols = names(first_boot)
  stopifnot(all_cols == names(original))
  relevant_cols = character(0)
  if (stat_type == "basic_stats") {
    relevant_cols = all_cols[grepl("^Mean_|^SD_|^P_", all_cols)]
  } else if (stat_type == "quantiles") {
    relevant_cols = all_cols[grepl("^Quant_", all_cols)]
  } else if (stat_type == "cafs") {
    relevant_cols = all_cols[grepl("^P_", all_cols)]
  } else if (stat_type == "delta_funs") {
    relevant_cols = all_cols[grepl("^Quant_|^Avg_|^Delta_", all_cols)]
  } else if (stat_type == "densities") {
    relevant_cols = all_cols[grepl("^Dens_", all_cols)]
  }
  stopifnot(length(relevant_cols) >= 1)


  alpha = 1 - level
  level_lower = alpha / 2
  level_upper = level + alpha/2

  calc_interval = function(matrix) {
    stopifnot(is.matrix(matrix))
    # rows -> statistic realizations
    # cols -> replications
    out <- apply(
      matrix, 1, quantile, probs = c(level_lower, level_upper), na.rm = TRUE
    )
    # returns probs as rows and statistic realizations as cols
    return(out)
  }

  n_rows = nrow(first_boot)
  interval = lapply(seq_along(relevant_cols), \(i){
    matrix = vapply(resample_list, \(one_entry) {
      return(one_entry[relevant_cols][[i]])
    }, FUN.VALUE = numeric(n_rows))
    return(calc_interval(matrix))
  })


  # now assemble

  # find the remaining column names and figure out where to insert the
  # new "Estimate" column
  all_other_cols = setdiff(all_cols, relevant_cols)
  to_create = c("lower", "orig", "upper")
  if ("Cond" %in% all_other_cols) {
    pos <- which(all_other_cols == "Cond")
  } else {
    pos <- which(all_other_cols == "Source")
  }
  stopifnot(length(pos) >= 1)
  stopifnot(pos >= 1)


  # create the lower, original, and upper data.frame
  result <- lapply(to_create, \(what) {
    estimate <- switch(
      what, lower = paste(level_lower*100, "%", sep = ""), orig = "orig",
      upper =  paste(level_upper*100, "%", sep = "")
    )
    dv_cols <- switch(
      what, lower = lapply(interval, `[`, 1, ), orig = original[relevant_cols],
      upper = lapply(interval, `[`, 2, )
    )
    template <- switch(
      what, lower = first_boot, upper = first_boot, orig = original
    )
    df <- template[all_other_cols]
    if (pos == ncol(df)) {
      df <- cbind(df, Estimate = estimate)
    } else {
      df <- cbind(df[1:pos], Estimate = estimate, df[(pos + 1):ncol(df)])
    }
    df[relevant_cols] <- dv_cols
    return(df)
  })

  # bind together and return (as a stats_dm object)
  result <- do.call("rbind", result)
  if ("Cond" %in% names(result)) {
    result <- result[order(result$Source, result$Cond),]
  } else {
    result <- result[order(result$Source),]
  }
  rownames(result) <- NULL
  # ensures that the source is always clear (when resampling under the model,
  # the bootstrapped data sets are treated as observed data)
  result$Source = unique(original$Source)
  result = copy_class_attributes(old = original, new = result)
  validate_stats_dm(result)

  return(result)
}

# CREATE stats_dm objects -------------------------------------------------


#' Create a New stats_dm Object
#'
#' @description
#' `new_stats_dm` initializes a `stats_dm` object to label statistic types and
#' store necessary attributes for the custom methods (such as `plot_*`)
#'
#'
#' @param stat_df a `data.frame`, containing calculated statistics to be
#' encapsulated within the `stats_dm` class.
#' @param type a character string, specifying the type of statistic provided by
#' `stat_df`. Valid options include `"basic_stats"`, `"cafs"`, `"quantiles"`,
#' `"delta_funs"`, and `"fit_stats"`.
#' @param ... Additional arguments passed to set attributes. For `"cafs"`,
#' `"quantiles"`, and `"delta_funs"`, a `b_coding` attribute is required.
#'
#' @details
#' `new_stats_dm` sets up the `stat_df` object by assigning it the class
#' `stats_dm`, along with additional classes based on the specified `type`.
#' For "basic_stats", "cafs", "quantiles", "delta_funs", this will be
#' c(`<type>`, "sum_dist", "stats_dm", "data.frame")". For "fit_stats", this
#' will be c("<type>", "stats_dm", "data.frame")".
#'
#' For basic stats, Conditional Accuracy Functions (CAFs), Quantiles, and
#' Delta Functions, the function requires a `b_coding` argument, which specifies
#' boundary coding details and is set as an attribute.
#'
#' The function performs validation through [dRiftDM::validate_stats_dm] to
#' ensure that the `stats_dm` object is well formatted.
#'
#' @return An object of class `stats_dm`, with additional classes and attributes
#' depending on `type`.
#'
#' @keywords internal
new_stats_dm <- function(stat_df, type, ...) {

  # if the option-flag skip_new_stats_dm was set, don't create an object
  # of this type (done for performance reasons during bootstrapping)
  if (isTRUE(stats.options("skip_new_stats_dm"))) return(stat_df)

  # input checks
  stopifnot(is.data.frame(stat_df))

  # type of stats
  type <- match.arg(arg = type, choices = drift_dm_stats_types())

  # turn all NaNs to NA (to have a consistent output for missing values
  stat_df[] <- lapply(stat_df, function(x) {
    x[is.nan(x)] <- NA
    return(x)
  })

  # define the stat_df object as an object of class stats_dm
  class(stat_df) <- c("stats_dm", "data.frame")

  # if it is a summary statistic
  # add b_coding and more info about object
  dots <- list(...)
  if (type %in% drift_dm_stats_types("sum_dist")) {
    b_coding <- dots$b_coding
    stopifnot(!is.null(b_coding))
    attr(stat_df, "b_coding") <- b_coding
    class(stat_df) <- c(type, "sum_dist", class(stat_df))
  } else {
    # else just add the object info
    class(stat_df) <- c(type, class(stat_df))
  }

  # check if everything is well
  stat_df <- validate_stats_dm(stat_df)
  return(stat_df)
}




# VALIDATE stats_dm objects -----------------------------------------------


#' Validate a stats_dm Object
#'
#' @description
#' `validate_stats_dm` is an internal (i.e., not exported) generic function to
#' ensure that `stats_dm` objects, as well as their specific subclasses
#' (`basic_stats`, `cafs`, `quantiles`, `delta_funs`, `sum_dist`, and
#' `fit_stats`), meet the necessary structural and column requirements. Each
#' method performs class-specific validation checks.
#'
#' @param stat_df A `data.frame` of class `stats_dm`, `basic_stats`, `cafs`,
#' `quantiles`, `delta_funs`, `sum_dist`, or `fit_stats` containing the
#' calculated statistics to be validated.
#'
#' @details
#' The validation process checks for required columns and structure based on the
#' class of `stat_df`. Each class has specific requirements:
#'
#' - **`validate_stats_dm.stats_dm`:** Ensures `stat_df` is a `data.frame`.
#' - **`validate_stats_dm.basic_stats`:** Checks for the presence of `"Cond"`,
#'   exactly two columns with prefix `"Mean_`, and exactly one column prefixed
#'   with `"P_"`
#' - **`validate_stats_dm.cafs`:** Checks for the presence of `"Bin"`, `"Cond"`,
#'   and exactly one column prefixed with `"P_"`
#' - **`validate_stats_dm.quantiles`:** Requires `"Prob"`, `"Cond"`, and exactly
#'   two columns prefixed with `"Quant_"`
#' - **`validate_stats_dm.delta_funs`:** Ensures `"Prob"` exists, at least two
#'   columns prefixed with `"Quant_"`, and at least one column  each `Avg_`
#'   and `Delta_`
#' - **`validate_stats_dm.delta_funs`:** Ensures `"Cond"`, `"Time"`, and
#'   `"Stat"` exists, and at least two column with `"Dens_"`.
#' - **`validate_stats_dm.sum_dist`:** Checks for a `"Source"` column. Here,
#'  it is also checked whether cell combinations appear equally often.
#' - **`validate_stats_dm.fit_stats`:** Checks for if the fit statistics
#'   summarize a log-likelihood cost function or the RMSE statistic. In the
#'   former case, the columns `"Log_Like"`, `"Neg_Log_Like"`, `"AIC"`, and `"BIC"`
#'   are expected. In the latter case, the columns `"RMSE_ms"` and `"RMSE_s"`
#'   are expected.
#'
#'
#' @return Returns the unmodified `stat_df` for convenience.
#'
#' @keywords internal
validate_stats_dm <- function(stat_df) {
  if (isTRUE(stats.options("skip_validate_stats_dm"))) return(stat_df)
  UseMethod("validate_stats_dm")
}


#' @rdname validate_stats_dm
#' @export
validate_stats_dm.basic_stats <- function(stat_df) {
  NextMethod() # to validate stats_dm objects


  if (!("Cond" %in% colnames(stat_df))) {
    stop("no column 'Cond' in stats_dm")
  }

  if (sum("Mean_" == substr(colnames(stat_df), 1, 5)) != 2) {
    stop("couldn't find two Mean_ columns")
  }

  if (sum("SD_" == substr(colnames(stat_df), 1, 3)) != 2) {
    stop("couldn't find two SD_ columns")
  }

  if (sum("P_" == substr(colnames(stat_df), 1, 2)) != 1) {
    stop("no unique column 'P_' in stats_dm")
  }

  return(stat_df)
}

#' @rdname validate_stats_dm
#' @export
validate_stats_dm.cafs <- function(stat_df) {
  NextMethod() # to validate sum_dist objects


  if (!("Bin" %in% colnames(stat_df))) {
    stop("no column 'Bin' in stats_dm")
  }

  if (!("Cond" %in% colnames(stat_df))) {
    stop("no column 'Cond' in stats_dm")
  }

  if (sum("P_" == substr(colnames(stat_df), 1, 2)) != 1) {
    stop("no unique column 'P_' in stats_dm")
  }
  return(stat_df)
}

#' @rdname validate_stats_dm
#' @export
validate_stats_dm.quantiles <- function(stat_df) {
  NextMethod() # to validate sum_dist objects

  if (!("Prob" %in% colnames(stat_df))) {
    stop("no column 'Prob' in stats_dm")
  }

  if (!("Cond" %in% colnames(stat_df))) {
    stop("no column 'Cond' in stats_dm")
  }

  if (sum("Quant_" == substr(colnames(stat_df), 1, 6)) != 2) {
    stop("couldn't find two Quant_ columns")
  }

  return(stat_df)
}

#' @rdname validate_stats_dm
#' @export
validate_stats_dm.delta_funs <- function(stat_df) {
  NextMethod() # to validate sum_dist objects

  if (!("Prob" %in% colnames(stat_df))) {
    stop("no column 'Prob' in stats_dm")
  }

  if (sum("Quant_" == substr(colnames(stat_df), 1, 6)) < 2) {
    stop("couldn't find at least two Quant_ columns")
  }

  if (sum("Avg_" == substr(colnames(stat_df), 1, 4)) < 1) {
    stop("couldn't find a Avg_ column")
  }
  if (sum("Delta_" == substr(colnames(stat_df), 1, 6)) < 1) {
    stop("couldn't find a Delta_ column")
  }
  return(stat_df)
}


#' @rdname validate_stats_dm
#' @export
validate_stats_dm.densities <- function(stat_df) {
  NextMethod() # to validate sum_dist objects


  if (!("Cond" %in% colnames(stat_df))) {
    stop("no column 'Cond' in stats_dm")
  }


  if (!("Stat" %in% colnames(stat_df))) {
    stop("no column 'Stat' in stats_dm")
  }

  if (!("Time" %in% colnames(stat_df))) {
    stop("no column 'Time' in stats_dm")
  }

  if (sum("Dens_" == substr(colnames(stat_df), 1, 5)) != 2) {
    stop("couldn't find two Dens_ columns")
  }
  return(stat_df)
}


#' @rdname validate_stats_dm
#' @export
validate_stats_dm.sum_dist <- function(stat_df) {
  NextMethod() # to validate stats_dm objects

  if (!("Source" %in% colnames(stat_df))) {
    stop("no column 'Source' in stats_dm")
  }
  if (is.null(attr(stat_df, "b_coding"))) {
    stop("no attribute b_coding in stats_dm")
  }

  # check for equal count data
  check_equal_n = c("ID", "Source", "Cond", "Prob", "Bin", "Estimate", "Stat", "Time")
  check_equal_n = intersect(names(stat_df), check_equal_n)
  counts = table(
    stat_df[, check_equal_n], useNA = "ifany"
  )
  counts = subset(as.data.frame(counts), Freq > 0)
  check = length(unique(counts$Freq)) == 1L
  if (!check) {
    stop("The number of data points across factor levels is not equal")
  }

  return(stat_df)
}



#' @rdname validate_stats_dm
#' @export
validate_stats_dm.fit_stats <- function(stat_df) {
  NextMethod() # to validate stats_dm objects

  cols <- colnames(stat_df)
  if ("Log_Like" %in% cols) {
    exp_col_names = c("Log_Like", "Neg_Log_Like", "AIC", "BIC")
  } else if ("RMSE_ms" %in% cols) {
    exp_col_names = c("RMSE_s", "RMSE_ms")
  } else {
    stop("stat_df type could not be identified")
  }

  if (!identical(exp_col_names, cols)) {
    stop("stats_dm object does not have the expected column names.\n",
         "Expected: ", paste(exp_col_names, collapse = ", "), "\n",
         "Found: ", paste(cols, collapse = ", "))
  }

  return(stat_df)
}


#' @rdname validate_stats_dm
#' @export
validate_stats_dm.stats_dm <- function(stat_df) {
  if (!is.data.frame(stat_df)) {
    stop("stats_dm object to validate is not of type data.frame")
  }
  if (nrow(stat_df) == 0) {
    stop("stats_dm object to validate has zero rows")
  }
  return(stat_df)
}





# AGGREGATE stats_dm OBJECTS ----------------------------------------------

#' Aggregate Statistics Across ID
#'
#' @description
#' `aggregate_stats` is a (not exported) function to aggregate
#' `stats_dm` objects across `ID`s. Since the column names may vary by the
#' statistic type, the behavior of aggregate depends on the subclass of
#' `stats_dm`.
#'
#' @param stat_df A `data.frame` of class `stats_dm` (see
#' [dRiftDM::new_stats_dm()])
#'
#' @details
#' `aggregate_stats` calls the [dRiftDM::internal_aggregate()] with the
#' relevant arguments
#'
#' @return Returns the statistics aggregated across the relevant cols.
#'
#' @seealso [dRiftDM::new_stats_dm], [dRiftDM::calc_stats],
#' [dRiftDM::internal_aggregate()]
#'
#' @keywords internal
aggregate_stats <- function(stat_df) {
  stopifnot(inherits(stat_df, "stats_dm"))
  internal_aggregate(
    data = stat_df,
    group_cols = c("Source", "Cond", "Estimate", "Bin", "Prob", "Stat", "Time")
  )
}



#' Aggregate Data Frame Columns by Group
#'
#' internal function to aggregate columns of a data frame across "ID"
#' while considering a set of grouping columns. It retains the class and
#' attriubtes of the input data.
#'
#' @param data A [data.frame] containing the data to be aggregated. It should
#'  include both the grouping columns, an "ID" column, and the columns for which
#'  aggregation shall take place.
#' @param group_cols A character vector specifying the names of the columns to
#'  group by during aggregation.
#'
#' @details
#' `internal_aggregate` identifies DV columns as those not in `group_cols` or
#' `"ID"`. It then calculates the mean of these DV columns, grouped by the
#' specified columns. Columns specified in `group_cols` that are not part of
#' `data` are ignored silently.
#'
#' @return A `data.frame` containing the aggregated data.
#'
#' @seealso [dRiftDM::aggregate_stats()], [dRiftDM::calc_stats()],
#' [dRiftDM::new_stats_dm()]
#'
#' @keywords internal
internal_aggregate <- function(data, group_cols) {
  all_cols <- colnames(data)
  for_agg <- unpack_obj(data)

  # Select columns that aren't grouping or ID columns
  dv_cols <- setdiff(all_cols, c("ID", group_cols))

  # Keep only present group columns
  group_cols <- intersect(all_cols, group_cols)

  # Capture original group combinations in their appearance order
  original_keys <- unique(for_agg[group_cols])

  # Perform aggregation (aggregate() sorts group columns internally)
  agg_df <- stats::aggregate(
    x = for_agg[dv_cols],
    by = for_agg[rev(group_cols)],  # reverse to keep grouping priority
    FUN = mean,
    na.rm = TRUE
  )

  # Reorder columns to match expected layout
  agg_df <- agg_df[c(group_cols, dv_cols)]

  # Reorder rows to match original order
  key_agg <- do.call(paste, agg_df[group_cols])
  key_orig <- do.call(paste, original_keys[group_cols])
  agg_df <- agg_df[match(key_orig, key_agg), ]
  rownames(agg_df) <- NULL

  # Keep class and attributes
  agg_df <- copy_class_attributes(old = data, new = agg_df)
  agg_df <- validate_stats_dm(agg_df)

  return(agg_df)
}



# HELPER TO KEEP CLASS/ATTRIBUTS  --------------------------------------------

#' Copy Class Attributes from One Object to Another
#'
#' This function transfers class attributes from an `old` object to a `new`
#' object, ensuring that `new` inherits the class structure and missing
#' attributes of `old`. The primary purpose is to enforce class consistency and
#' restore any lost attributes when modifying or combining objects. It is
#' used in the internals of the package and it is not exported.
#'
#' @param old The source object from which class attributes will be copied.
#' @param new The target object to which class attributes will be assigned.
#' @return The modified `new` object with attributes and class from `old`.
#'
#' @details
#' The function assumes that all class attributes of `new` can be found in
#' `old`. Note also, that the order of attributes is not ensured.
#'
#' @keywords internal
copy_class_attributes <- function(old, new) {
  UseMethod("copy_class_attributes")
}


#' @rdname copy_class_attributes
#' @export
copy_class_attributes.stats_dm <- function(old, new) {
  stopifnot(class(new) %in% class(old))
  lost_attribtues <- setdiff(names(attributes(old)), names(attributes(new)))

  class(new) <- class(old) # ensures sorting
  for (one_attr in lost_attribtues) { # doesn't ensure sorting
    attr(new, one_attr) <- attr(old, one_attr)
  }

  return(new)
}




# HELPER TO TEMPORARILY SET OPTIONS ---------------------------------------

#' Helper to get, set, or reset package-global options for statistics
#'
#' Internal utility to manage global options for the package via the
#' "stats.dRiftDM" option slot.
#'
#' @param ... input, see Details below
#'
#' @details
#'
#' Usage patterns:
#' - `stats.options()`:
#'     Returns the full list of currently stored options.
#' - `stats.options(name)`:
#'     Returns the value of a specific option (must be a single unnamed string).
#' - `stats.options(name = value, ...)`:
#'     Sets (or updates) named option(s).
#' - `stats.options(NULL)`:
#'     Resets (clears) the entire option list.
#'
#' This function is intended for internal use only.
#' It behaves similarly to [options()] and keeps all package-specific options
#' in a single named list under `getOption("stats.dRiftDM")`.
#'
#' Setting an argument can only be done once with this function, any additional
#' attempts to modify an option will not work (unless this argument is
#' explicitly set to `NULL`).
#'
#' @return Depending on usage:
#' - Full list of options (if no input),
#' - A specific option value (if string input),
#' - Invisibly `NULL` (if setting or resetting options).
#' @internal
stats.options <- function(...) {
  dots <- list(...)
  args <- getOption("stats.dRiftDM")

  # No input: return full current option list
  if (length(dots) == 0) return(args)

  # Named arguments: update options
  if (!is.null(names(dots)) && all(nzchar(names(dots)))) {
    updated_args <- args
    for (nm in names(dots)) {
      if (is.null(dots[[nm]])) {
        updated_args[[nm]] <- NULL  # explicitly remove it
      } else if (!(nm %in% names(updated_args))) {
        updated_args[[nm]] <- dots[[nm]]  # only add new elements
      }
    }
    options("stats.dRiftDM" = updated_args)
    return(invisible(NULL))
  }

  # Unnamed single argument: retrieve specific entry or reset everything
  stopifnot(length(dots) == 1)
  input <- dots[[1]]

  # if input was NULL, then reset everything
  if (is.null(input)) {
    options("stats.dRiftDM" = NULL)
    return(invisible(NULL))
  }

  # if input was not NULL, check if it is a single character and retrieve entry
  stopifnot(is.character(input), length(input) == 1)
  return(args[[input]])
}

# UNPACK METHODS ----------------------------------------------------------

#' @rdname unpack_obj
#' @export
unpack_obj.stats_dm <- function(object, ..., unpack_elements = TRUE) {
  if (unpack_elements) {
    object <- as.data.frame(object)
    attr(object, "b_coding") <- NULL
  }

  return(object)
}


#' @rdname unpack_obj
#' @export
unpack_obj.stats_dm_list <- function(object, ..., unpack_elements = TRUE,
                                     type = NULL) {
  # default is all stored stat types
  if (is.null(type)) {
    type <- names(object)
  }
  type <- match.arg(type, names(object), several.ok = TRUE)

  # iterate across all
  stats <- sapply(type, function(x) {
    unpack_obj(object[[x]], unpack_elements = unpack_elements)
  }, simplify = FALSE, USE.NAMES = TRUE)

  if (length(type) == 1) {
    return(stats[[1]])
  } else {
    return(stats)
  }
}
